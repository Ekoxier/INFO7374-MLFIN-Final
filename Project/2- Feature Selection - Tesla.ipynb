{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2ae625d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (from ta) (1.20.3)\n",
      "Requirement already satisfied: pandas in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (from ta) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (from pandas->ta) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinav-m/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "617e7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/jbd9m2gj61n3fk0zl36lyp940000gn/T/ipykernel_7995/1363730320.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tsla['direction'][0] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- BKCN: No timezone found, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "                                  Open         High          Low        Close  \\\n",
      "Date                                                                            \n",
      "2021-03-11 00:00:00+08:00  5024.560059  5138.410156  5020.580078  5128.220215   \n",
      "2021-03-12 00:00:00+08:00  5153.669922  5153.669922  5086.819824  5146.379883   \n",
      "2021-03-15 00:00:00+08:00  5116.120117  5120.879883  4992.399902  5035.540039   \n",
      "2021-03-16 00:00:00+08:00  5054.410156  5084.310059  5009.950195  5079.359863   \n",
      "2021-03-17 00:00:00+08:00  5062.770020  5123.549805  5020.129883  5100.859863   \n",
      "\n",
      "                             Adj Close  Volume  \n",
      "Date                                            \n",
      "2021-03-11 00:00:00+08:00  5128.220215  189600  \n",
      "2021-03-12 00:00:00+08:00  5146.379883  201000  \n",
      "2021-03-15 00:00:00+08:00  5035.540039  204200  \n",
      "2021-03-16 00:00:00+08:00  5079.359863  161400  \n",
      "2021-03-17 00:00:00+08:00  5100.859863  149200  \n",
      "Empty DataFrame\n",
      "Columns: [Open, High, Low, Close, Adj Close, Volume]\n",
      "Index: []\n",
      "<bound method NDFrame.head of Empty DataFrame\n",
      "Columns: [SP_500_Adj_Close, AAPL_Adj_Close, AMZN_Adj_Close, GOOG_Adj_Close, CMA_Adj_Close, BTC_Adj_Close, ETH_Adj_Close, XRP_Adj_Close, LTC_Adj_Close, ADA_Adj_Close, Fama_French_Mkt_RF, Fama_French_SMB, Fama_French_HML, OBV, mom_5_20, mom_20_100, mom_60_200, TSLA_CLOSE, VIX_IDX, avg_close_20_days_, avg_Close_50_days, ADS_INDEX, bkcn_Adj_Close, shsz300_df, TSLA_RSI, TSLA_MACD]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 26 columns]>\n",
      "(0, 26)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 26)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2s/jbd9m2gj61n3fk0zl36lyp940000gn/T/ipykernel_7995/1363730320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_regressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \"\"\"\n\u001b[1;32m    765\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[1;32m    767\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    727\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 26)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import getFamaFrenchFactors as gff\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "\n",
    "# Define the ticker symbol for Tesla\n",
    "ticker = 'TSLA'\n",
    "\n",
    "# Define the start and end dates for the data\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-01-01'\n",
    "\n",
    "# Download the data from Yahoo Finance\n",
    "tsla = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "\n",
    "# Import ADS_INDEX\n",
    "from datetime import datetime\n",
    "date_parser = lambda x: datetime.strptime(x, '%Y:%m:%d')\n",
    "ads_data = pd.read_excel(\"ads_index_010622.xlsx\", parse_dates=['Date'], date_parser=date_parser)\n",
    "new_ads_data = ads_data[['Date', 'ADS_INDEX_010622']].set_index('Date')\n",
    "\n",
    "# Calculate OBV\n",
    "tsla['daily_return'] = tsla['Adj Close'].pct_change()\n",
    "tsla['direction'] = np.where(tsla['daily_return'] >= 0, 1, -1)\n",
    "tsla['direction'][0] = 0\n",
    "tsla['vol_adjusted'] = tsla['Volume'] * tsla['direction']\n",
    "tsla['OBV'] = tsla['vol_adjusted'].cumsum()\n",
    "\n",
    "\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
    "aapl = yf.download('AAPL', start=start_date, end=end_date)\n",
    "amzn = yf.download('AMZN', start=start_date, end=end_date)\n",
    "goog = yf.download('GOOG', start=start_date, end=end_date)\n",
    "cma = yf.download('CMA', start=start_date, end=end_date)\n",
    "btc = yf.download('BTC-USD', start=start_date, end=end_date)\n",
    "eth = yf.download('ETH-USD', start=start_date, end=end_date)\n",
    "xrp = yf.download('XRP-USD', start=start_date, end=end_date)\n",
    "ltc = yf.download('LTC-USD', start=start_date, end=end_date)\n",
    "ada = yf.download('ADA-USD', start=start_date, end=end_date)\n",
    "vix = yf.download('^VIX', start=start_date, end=end_date)\n",
    "\n",
    "# Calculate additional features\n",
    "tsla['mom_5_20'] = (tsla['Close'] / tsla['Close'].shift(5)) - 1\n",
    "tsla['mom_20_100'] = (tsla['Close'] / tsla['Close'].shift(20)) - 1\n",
    "tsla['mom_60_200'] = (tsla['Close'] / tsla['Close'].shift(60)) - 1\n",
    "\n",
    "bkcn_df = yf.download(\"BKCN\", start_date, end_date)\n",
    "shsz300_df = yf.download(\"000300.SS\", start_date, end_date)\n",
    "rsi_indicator = RSIIndicator(close=tsla['Adj Close'], window=14)\n",
    "macd_indicator = MACD(close=tsla['Adj Close'])\n",
    "\n",
    "tsla['rsi'] = rsi_indicator.rsi()\n",
    "tsla['macd'] = macd_indicator.macd()\n",
    "\n",
    "print(shsz300_df.head())\n",
    "print(bkcn_df.head())\n",
    "\n",
    "# Calculate Fama French 3 factors\n",
    "ff_data = gff.famaFrench3Factor(frequency='m') \n",
    "ff_data.rename(columns={\"date_ff_factors\": 'Date'}, inplace=True)\n",
    "ff_data.set_index('Date',inplace=True)\n",
    "ff_data = ff_data.resample('D').interpolate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reset index and convert all dates to same timezones, so they become mergable\n",
    "for x in [sp500,aapl,amzn,goog,tsla,cma,btc,eth,xrp,ltc,ada,ff_data,vix,new_ads_data,shsz300_df,bkcn_df]:\n",
    "    x.reset_index(inplace=True)\n",
    "    x['Date'] =  pd.to_datetime(x['Date']).dt.date\n",
    "\n",
    "tsla = ff_data.merge(tsla,on='Date')\n",
    "tsla = new_ads_data.merge(tsla,on='Date')\n",
    "    \n",
    "## CORRECTION - Need to merge FF data according to date here\n",
    "\n",
    "df_regressor = pd.DataFrame({\n",
    "    'SP_500_Adj_Close':sp500['Adj Close'].shift(1),\n",
    "    'AAPL_Adj_Close':aapl['Adj Close'].shift(1),\n",
    "    'AMZN_Adj_Close':amzn['Adj Close'].shift(1),\n",
    "    'GOOG_Adj_Close':goog['Adj Close'].shift(1),\n",
    "    'CMA_Adj_Close':cma['Adj Close'].shift(1),\n",
    "    'BTC_Adj_Close':btc['Adj Close'].shift(1),\n",
    "    'ETH_Adj_Close':eth['Adj Close'].shift(1),\n",
    "    'XRP_Adj_Close':xrp['Adj Close'].shift(1),\n",
    "    'LTC_Adj_Close':ltc['Adj Close'].shift(1),\n",
    "    'ADA_Adj_Close':ada['Adj Close'].shift(1),\n",
    "    'Fama_French_Mkt_RF':tsla['Mkt-RF'].shift(1),\n",
    "    'Fama_French_SMB' : tsla['SMB'].shift(1),\n",
    "    'Fama_French_HML' : tsla['HML'].shift(1),\n",
    "    'OBV': tsla['OBV'].shift(1),\n",
    "    'mom_5_20':  tsla['mom_5_20'].shift(1),\n",
    "    'mom_20_100':  tsla['mom_20_100'].shift(1),\n",
    "    'mom_60_200':  tsla['mom_60_200'].shift(1),\n",
    "     'TSLA_CLOSE': tsla['Adj Close'],\n",
    "    'VIX_IDX':vix['Adj Close'].shift(1),\n",
    "    'avg_close_20_days_': tsla['Adj Close'].rolling(window=20).mean().shift(1),\n",
    "    'avg_Close_50_days':tsla['Adj Close'].rolling(window=50).mean().shift(1),\n",
    "    'ADS_INDEX': tsla['ADS_INDEX_010622'].shift(1),\n",
    "#     'bkcn_Adj_Close':bkcn_df['Adj Close'].shift(1),\n",
    "    'shsz300_df':shsz300_df['Adj Close'].shift(1),\n",
    "    \n",
    "    'TSLA_RSI': tsla['rsi'].shift(1),\n",
    "    'TSLA_MACD':tsla['macd'].shift(1)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove any rows with missing data\n",
    "df_regressor.dropna(inplace=True)\n",
    "print(df_regressor.head)\n",
    "print(df_regressor.shape)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_regressor)\n",
    "# Split the data into features (X) and target (y)\n",
    "y = df_regressor['TSLA_CLOSE']\n",
    "df_regressor = df_regressor.drop(columns=['TSLA_CLOSE'],axis=1)\n",
    "X = df_regressor\n",
    "# Fit a decision tree model to the data\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Select the top 10 most important features\n",
    "selector_Decision_Trees = SelectFromModel(tree, prefit=True, threshold=-np.inf, max_features=10)\n",
    "selected_features = X.columns[selector_Decision_Trees.get_support()]\n",
    "\n",
    "final_feature_set = set()\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 most important features through Decision Trees:')\n",
    "for feature in selected_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81dab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Select the top 10 most important features\n",
    "selector_RIDGE = SelectFromModel(ridge, prefit=True, threshold=-np.inf, max_features=10)\n",
    "selected_features = X.columns[selector_RIDGE.get_support()]\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 most important features (Ridge Regression):')\n",
    "for feature in selected_features:\n",
    "      print('-', feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e34e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a LassoLarsCV model to the data\n",
    "lars = LassoLarsCV(cv=5).fit(X, y)\n",
    "\n",
    "# Select the top 10 most important features\n",
    "selector_LARS = SelectFromModel(lars, prefit=True, threshold=-np.inf, max_features=10)\n",
    "selected_features_LARS = X.columns[selector_LARS.get_support()]\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 most important features (LARS):')\n",
    "for feature in selected_features_LARS:\n",
    "      print('-', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a RandomForest model to the data\n",
    "rf = RandomForestRegressor(random_state=0, n_estimators=100).fit(X, y)\n",
    "\n",
    "# Select the top 10 most important features\n",
    "selector_RF = SelectFromModel(rf, prefit=True, threshold=-np.inf, max_features=10)\n",
    "selected_features = X.columns[selector_RF.get_support()]\n",
    "# selector.estimator.columns\n",
    "# selector.threshold_\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 most important features (RandomForest):')\n",
    "for feature in selected_features:\n",
    "    print('-', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an XGBoost model to the data\n",
    "xgb_model = xgb.XGBRegressor(random_state=0, n_estimators=100).fit(X, y)\n",
    "\n",
    "# Select the top 10 most important features\n",
    "selector_XGB = SelectFromModel(xgb_model, prefit=True, threshold=-np.inf, max_features=10)\n",
    "selected_features = X.columns[selector_XGB.get_support()]\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print('Top 10 most important features (XGBoost):')\n",
    "for feature in selected_features:\n",
    "    print('-', feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24816e20",
   "metadata": {},
   "source": [
    "## Doubts\n",
    "1. KDE Estimation - Enough just plotting the stock or actual KDE estimation\n",
    "2. More feature suggestions / Momentum factors \n",
    "\n",
    "3. Class sample (0408) - Why fit linear regression after feature importance?? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f500b",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- Normalize data before KDE estimation\n",
    "- Graphs / Subplots for different features - Price vs MacD etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3033958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_features_with_importance(title,selector,n=10):\n",
    "    features_with_importance = sorted(list(zip(selector.estimator.feature_names_in_,selector.estimator.feature_importances_)),key = lambda x: x[1],reverse=True)\n",
    "    features_with_importance = features_with_importance[:n]\n",
    "    for f_name,f_imp in features_with_importance[:3]:\n",
    "        final_feature_set.add(f_name)\n",
    "    df = pd.DataFrame(data={\"Feature Name\":[x[0] for x in features_with_importance],\"Coefficient / Importance\":[x[1] for x in features_with_importance]},columns=[\"Feature Name\",\"Coefficient / Importance\"])\n",
    "    plt.figure(figsize = ( 20 , 10 ))\n",
    "    sns.barplot(x=df['Feature Name'],y=df[\"Coefficient / Importance\"]) \n",
    "    # Set title for figure\n",
    "    plt.title( title , size = 24 )\n",
    "    # Display figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ee9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_with_importance(\"XG Boost Feature importance\",selector_XGB,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_features_with_importance_Regressions(title,selector,n=10):\n",
    "    features_with_importance = sorted(list(zip(X.columns,selector.estimator.coef_)),key = lambda x: x[1],reverse=True)\n",
    "    features_with_importance = features_with_importance[:n]\n",
    "    for f_name,f_imp in features_with_importance[:3]:\n",
    "        final_feature_set.add(f_name)\n",
    "    df = pd.DataFrame(data={\"Feature Name\":[x[0] for x in features_with_importance],\"Coefficient / Importance\":[x[1] for x in features_with_importance]},columns=[\"Feature Name\",\"Coefficient / Importance\"])\n",
    "    plt.figure(figsize = ( 20 , 10 ))\n",
    "    sns.barplot(x=df['Feature Name'],y=df[\"Coefficient / Importance\"]) \n",
    "    # Set title for figure\n",
    "    plt.title( title , size = 24 )\n",
    "    # Display figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_with_importance_Random_forest(title,selector,n=10):\n",
    "    features_with_importance = sorted(list(zip(X.columns,selector.estimator.feature_importances_)),key = lambda x: x[1],reverse=True)\n",
    "    features_with_importance = features_with_importance[:n]\n",
    "    for f_name,f_imp in features_with_importance[:3]:\n",
    "        final_feature_set.add(f_name)\n",
    "    df = pd.DataFrame(data={\"Feature Name\":[x[0] for x in features_with_importance],\"Coefficient / Importance\":[x[1] for x in features_with_importance]},columns=[\"Feature Name\",\"Coefficient / Importance\"])\n",
    "    plt.figure(figsize = ( 20 , 10 ))\n",
    "    sns.barplot(x=df['Feature Name'],y=df[\"Coefficient / Importance\"]) \n",
    "    # Set title for figure\n",
    "    plt.title( title , size = 24 )\n",
    "    # Display figure\n",
    "    plt.show()\n",
    "    selector_RF.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_with_importance_Regressions(\"LARS Feature importance\",selector_LARS,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_with_importance_Regressions(\"Ridge Regression Feature importance\",selector_RIDGE,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca33511",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_with_importance_Random_forest(\"Random Forest Feature importance\",selector_RF,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f891df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_with_importance_Random_forest(\"Decision Trees Feature importance\",selector_RF,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c60ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_feature_set)\n",
    "print(len(final_feature_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
